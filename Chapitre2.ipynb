{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "902859c9-22c4-4d8e-ae86-4807a9d50784",
   "metadata": {},
   "source": [
    "# Exercice Chap 2\n",
    "#### 1: Lorem Ipsum is just a random txt that devs use as a placeholder for multiple things (especially web developping) when you don't have the real text and just want to test your functionnality. Put a Lorem Ipsum of 3 paragraphs in a txt file using python, each paragraph delimited by two new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "292ce2e9-4c87-4083-8c5f-a7532bb61934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unde corrupti fuga eum, porro sequi placeat inventore ex. Eos nihil architecto, nam accusantium dicta perspiciatis beatae dolorem cupiditate error labore a, odio laudantium repellat a nihil quasi vero, repellendus labore animi sapiente necessitatibus sint enim libero quia. Enim rem minima mollitia, deserunt eaque natus?\n",
      "\n",
      "\n",
      "Voluptatum ex a mollitia fugiat id ad velit labore iusto et, eum nemo quae voluptate, pariatur asperiores cupiditate sit iure vitae, nesciunt nulla doloremque inventore? Id nostrum error quidem beatae neque ex deleniti inventore, nobis delectus sint facere aliquam pariatur natus fuga cupiditate dicta praesentium obcaecati, modi vel quaerat natus esse voluptatibus quo reprehenderit repellendus dolorem vitae. Ad dolorum minima eius, dolores qui odio repellendus ex magnam exercitationem possimus dolorum deleniti, odio voluptatem debitis quos vero iste repudiandae beatae culpa et doloremque. Animi eos quasi quae molestias porro dolores consequuntur, dolorem culpa totam itaque molestiae nemo minima eum, officiis mollitia alias distinctio corrupti pariatur aliquam.\n",
      "\n",
      "\n",
      "Velit dolor quibusdam similique ipsum nulla, assumenda magnam sunt alias, ab magnam officia dignissimos, maiores provident nemo nobis voluptatum aliquam distinctio nam placeat sit tempore quam, asperiores atque minus? Mollitia eos autem obcaecati iusto debitis sint officiis illo quos natus accusamus, animi voluptate dolor, modi architecto natus, tenetur distinctio earum doloremque expedita vitae, temporibus molestiae nobis voluptate est assumenda. Repudiandae consequatur itaque vel quidem, nihil obcaecati quis.\n"
     ]
    }
   ],
   "source": [
    "from lorem_text import lorem\n",
    "\n",
    "\n",
    "# Generate Lorem Ipsum text for three paragraphs\n",
    "paragraphs = [lorem.paragraph() for _ in range(3)]\n",
    "\n",
    "# Join the paragraphs with two spaces\n",
    "li = '\\n\\n\\n'.join(paragraphs)\n",
    "\n",
    "print(li)\n",
    "\n",
    "# Write the Lorem Ipsum text to a file\n",
    "with open(\"C:/Users/user/OneDrive/Documents/M1-APE-DS2E/Infrastructure_de_donnes/NoSQL/o1o.txt\", 'w') as file:\n",
    "    file.write(li)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ba090-8f23-458b-878f-3f346e96ea8a",
   "metadata": {},
   "source": [
    "#### 2: Update the txt file by removing the first paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49c84fc1-a611-4323-928f-e37c292cdc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " '\\n',\n",
       " 'Voluptatum ex a mollitia fugiat id ad velit labore iusto et, eum nemo quae voluptate, pariatur asperiores cupiditate sit iure vitae, nesciunt nulla doloremque inventore? Id nostrum error quidem beatae neque ex deleniti inventore, nobis delectus sint facere aliquam pariatur natus fuga cupiditate dicta praesentium obcaecati, modi vel quaerat natus esse voluptatibus quo reprehenderit repellendus dolorem vitae. Ad dolorum minima eius, dolores qui odio repellendus ex magnam exercitationem possimus dolorum deleniti, odio voluptatem debitis quos vero iste repudiandae beatae culpa et doloremque. Animi eos quasi quae molestias porro dolores consequuntur, dolorem culpa totam itaque molestiae nemo minima eum, officiis mollitia alias distinctio corrupti pariatur aliquam.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Velit dolor quibusdam similique ipsum nulla, assumenda magnam sunt alias, ab magnam officia dignissimos, maiores provident nemo nobis voluptatum aliquam distinctio nam placeat sit tempore quam, asperiores atque minus? Mollitia eos autem obcaecati iusto debitis sint officiis illo quos natus accusamus, animi voluptate dolor, modi architecto natus, tenetur distinctio earum doloremque expedita vitae, temporibus molestiae nobis voluptate est assumenda. Repudiandae consequatur itaque vel quidem, nihil obcaecati quis.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the content of the file\n",
    "file_path = \"C:/Users/user/OneDrive/Documents/M1-APE-DS2E/Infrastructure_de_donnes/NoSQL/o1o.txt\"\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.readlines()\n",
    "\n",
    "content.pop(0)\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    for cont in content:\n",
    "        file.write(cont)\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.readlines()\n",
    "\n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620c6373-dc24-4a5c-8385-07ae41125e58",
   "metadata": {},
   "source": [
    "#### 3: Create a dict from the paper of lecun et al. and goodfellow et al. with authors, title, affiliations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7baac1b-f530-43a3-ae7a-d0fb15418bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'authors': ['Yann LeCun, Yoshua Bengio, Geoffrey Hinton'], 'title': 'Deep Learning', 'affiliations': ['New York University', 'Université de Montréal,', 'University of Toronto']}\n"
     ]
    }
   ],
   "source": [
    "paper = {\"authors\" : [\"Yann LeCun, Yoshua Bengio, Geoffrey Hinton\"],\n",
    "         \"title\" : \"Deep Learning\",\n",
    "         \"affiliations\" : [\"New York University\",\"Université de Montréal,\",\"University of Toronto\"]}\n",
    "\n",
    "print(paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f5d0a-98cf-4901-b313-cb85a8a38bdf",
   "metadata": {},
   "source": [
    "#### 4: Save the previously created dict in the JSON format and load it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1808c12-c896-4345-8a95-8b48f49f0b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"C:/Users/user/OneDrive/Documents/M1-APE-DS2E/Infrastructure_de_donnes/NoSQL/paper.json\", 'w') as fp:\n",
    "    json.dump(paper, fp)\n",
    "    \n",
    "with open('C:/Users/user/OneDrive/Documents/M1-APE-DS2E/Infrastructure_de_donnes/NoSQL/data.json', 'r') as fp:\n",
    "    test= json.load(fp)\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb6f2e0-3778-41f9-9f8b-2ed95a643438",
   "metadata": {},
   "source": [
    "#### 5: Save the previously created dict in the pickle format. Try to open manually (i.e with a text editor), is it human readable ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bbc1de9-1448-411d-82d2-30f4a7b1ab5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x80\\x04\\x95\\xb4\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x94(\\x8c\\x07authors\\x94]\\x94\\x8c*Yann LeCun, Yoshua Bengio, Geoffrey Hinton\\x94a\\x8c\\x05title\\x94\\x8c\\rDeep Learning\\x94\\x8c\\x0caffiliations\\x94]\\x94(\\x8c\\x13New York University\\x94\\x8c\\x19Universit\\xc3\\xa9 de Montr\\xc3\\xa9al,\\x94\\x8c\\x15University of Toronto\\x94eu.'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('C:/Users/user/OneDrive/Documents/M1-APE-DS2E/Infrastructure_de_donnes/NoSQL/paper.pickle', 'wb') as file:\n",
    "    pickle.dump(paper, file)\n",
    "with open('C:/Users/user/OneDrive/Documents/M1-APE-DS2E/Infrastructure_de_donnes/NoSQL/paper.pickle', 'rb') as file:\n",
    "    contents = file.read()\n",
    "\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42f75f4-6607-4777-81dd-05ba556d3005",
   "metadata": {},
   "source": [
    "#### 6: Parse the xml_file2 in the same way as in the lecture. put infos in a dict and save it in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47bda8b0-ef1d-4b57-9e69-7663a647bfa9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "not well-formed (invalid token): line 1, column 0 (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3553\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[31], line 4\u001b[0m\n    tree = ET.parse('C:/Users/user/OneDrive/Documents/M1-APE-DS2E/Infrastructure_de_donnes/NoSQL/xml_file2.nxml')\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\xml\\etree\\ElementTree.py:1204\u001b[0m in \u001b[0;35mparse\u001b[0m\n    tree.parse(source, parser)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\xml\\etree\\ElementTree.py:569\u001b[1;36m in \u001b[1;35mparse\u001b[1;36m\n\u001b[1;33m    self._root = parser._parse_whole(source)\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<string>\u001b[1;36m\u001b[0m\n\u001b[1;31mParseError\u001b[0m\u001b[1;31m:\u001b[0m not well-formed (invalid token): line 1, column 0\n"
     ]
    }
   ],
   "source": [
    "import lxml.etree\n",
    "\n",
    "xml_file = \"C:/Users/user/OneDrive/Documents/M1-APE-DS2E/Infrastructure_de_donnes/NoSQL/xml_file2.nxml\"\n",
    "root = lxml.etree.parse(xml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ed44cd9-7c68-4213-8d15-bc1f0996cd57",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m date \u001b[38;5;241m=\u001b[39m \u001b[43mroot\u001b[49m\u001b[38;5;241m.\u001b[39mxpath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//date//text()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m hour \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m.\u001b[39mxpath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//hour//text()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m to \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m.\u001b[39mxpath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//to//text()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'root' is not defined"
     ]
    }
   ],
   "source": [
    "date = root.xpath(\"//date//text()\")\n",
    "hour = root.xpath(\"//hour//text()\")\n",
    "to = root.xpath(\"//to//text()\")\n",
    "fromm = root.xpath(\"//from//text()\")\n",
    "body = root.xpath(\"//body//text()\")\n",
    "\n",
    "dict = {'date' : date, 'hour' : hour, 'to' : to, 'from' : fromm, 'body' : body}\n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f412c1-e45d-47d0-b574-eb87666080f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"C:/Users/user/OneDrive/Documents/M1-APE-DS2E/Infrastructure_de_donnes/NoSQL/xml_file2.nxml\", 'w') as fp:\n",
    "    json.dump(dict, fp)\n",
    "    \n",
    "with open('C:/Users/user/OneDrive/Documents/M1-APE-DS2E/Infrastructure_de_donnes/NoSQL/xml_file2.nxml', 'r') as fp:\n",
    "    xml_2 = json.load(fp)\n",
    "\n",
    "print(xml_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5aa8b-b504-4dfd-a8aa-c08f617d99a2",
   "metadata": {},
   "source": [
    "#### 7: Download an image of your choice and save it in either jpg or png."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd7666f6-00a2-4c15-860f-9cc76e438400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "im = Image.open(requests.get(\"https://imgs.search.brave.com/uepsM-WFJdb1vzcAzN7WPQz4LtmCbqbga5rg7nPnf8Y/rs:fit:860:0:0/g:ce/aHR0cHM6Ly9pLnNj/ZG4uY28vaW1hZ2Uv/YWI2NzYxNmQwMDAw/MWUwMmUxNzVhMTll/NTMwYzg5OGQxNjdk/MzliZg\", stream=True).raw)\n",
    "im.save(\"C:/Users/user/OneDrive/Documents/M1-APE-DS2E/Infrastructure_de_donnes/NoSQL/nirvana.png\", \"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef804a92-4e06-4ade-87bd-bf5f2772235b",
   "metadata": {},
   "source": [
    "#### 8: From the data/Chap2/data_world.json file, create a set of publisher type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fd49887-370b-4bbf-8a12-0898d1c869fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"C:/Users/user/OneDrive/Documents/M1-APE-DS2E/Infrastructure_de_donnes/NoSQL/data_world.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "publisher_types = set()\n",
    "for book in data:\n",
    "    if 'publisher_type' in book:\n",
    "        publisher_types.add(book['publisher_type'])\n",
    "\n",
    "print(publisher_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3c58e5-f0ed-4bc0-91f1-6c5abb5fabda",
   "metadata": {},
   "source": [
    "#### 9: From the data/Chap2/data_world.json file, delete the key of your choice and save the new dict as data_world_cleaned.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a03e240-c1b7-45e6-aa3c-ee8a8b7dd4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 5000/5000 [00:00<00:00, 553732.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Temperature profiles from MBT casts from the GRESHAM from Ocean Weather Station N (OWS-N) in the North Pacific Ocean from 07 February 1967 to 28 February 1967 (NCEI Accession 6700065)', 'accessLevel': 'public', 'accrualPeriodicity': 'irregular', 'contactPoint': {'@type': 'vcard:Contact', 'fn': 'DOC/NOAA/NESDIS/NCEI > National Centers for Environmental Information, NESDIS, NOAA, U.S. Department of Commerce', 'hasEmail': 'mailto:NODC.Services@noaa.gov'}, 'description': 'Bathythermograph data were collected from the GRESHAM within a 1-mile radius of Ocean Weather Station N (3000N 14000W) and in transit. Data were collected by the United States Coast Guard from 07 Februay 1967 to 28 February 1967. The platform was equipped and staffed to observe weather and sea conditions. Data were processed by NODC to the NODC standard Universal Bathythermograph Output (UBT) format. Full format description is available from NODC at www.nodc.noaa.gov/General/NODC-Archive/bt.html.\\n\\nThe UBT file format is used for temperature-depth profile data obtained using the mechanical bathythermograph (MBT) instrument. The maximum depth of MBT observations is approximately 285 m. Therefore, MBT data are useful only in studying the thermal structure of the upper layers of the ocean. Cruise information, date, position, and time are reported for each observation. The data record comprises pairs of temperature-depth values. Temperature data in this file are recorded at uniform 5 m depth intervals.', 'identifier': 'gov.noaa.nodc:6700065', 'keyword': ['DOC/NOAA/NESDIS/NODC > National Oceanographic Data Center', 'NESDIS', 'NOAA', 'U.S. Department of Commerce', 'DOC/NOAA/NESDIS/NCEI > National Centers for Environmental Information', 'NESDIS', 'NOAA', 'U.S. Department of Commerce', 'oceanography'], 'modified': '2010-12-17', 'publisher': {'@type': 'org:Organization', 'name': 'DOC/NOAA/NESDIS/NCEI > National Centers for Environmental Information, NESDIS, NOAA, U.S. Department of Commerce (Point of Contact)'}, 'spatial': '{\"type\": \"Polygon\", \"coordinates\": [[[-140.2, 29.8], [-124.5, 29.8], [-124.5, 37.0], [-140.2, 37.0], [-140.2, 29.8]]]}', 'temporal': '1967-02-07T00:00:00/1967-02-28T00:00:00', 'distribution': [{'@type': 'dcat:Distribution', 'downloadURL': 'http://accession.nodc.noaa.gov/6700065', 'mediaType': 'text/html', 'title': 'Details'}, {'@type': 'dcat:Distribution', 'downloadURL': 'http://accession.nodc.noaa.gov/oas/6700065', 'mediaType': 'text/html', 'title': 'Metadata'}, {'@type': 'dcat:Distribution', 'downloadURL': 'http://accession.nodc.noaa.gov/download/6700065', 'mediaType': 'text/html', 'title': 'Download'}, {'@type': 'dcat:Distribution', 'downloadURL': 'ftp://ftp.nodc.noaa.gov/nodc/archive/arc0001/6700065/', 'mediaType': 'text/html', 'title': 'FTP'}, {'@type': 'dcat:Distribution', 'downloadURL': 'http://www.ncei.noaa.gov/', 'mediaType': 'text/html', 'title': 'NOAA National Centers for Environmental Information website'}, {'@type': 'dcat:Distribution', 'downloadURL': 'http://www.nodc.noaa.gov/', 'mediaType': 'text/html', 'title': 'NOAA National Oceanographic Data Center website'}, {'@type': 'dcat:Distribution', 'downloadURL': 'http://www.nodc.noaa.gov', 'mediaType': 'text/html', 'title': 'US National Oceanographic Data Center website'}, {'@type': 'dcat:Distribution', 'downloadURL': 'http://gcmd.gsfc.nasa.gov/learn/keywords.html', 'mediaType': 'text/html', 'title': 'NASA GCMD Keyword Community Page'}], 'bureauCode': ['006:48'], 'programCode': ['006:059'], 'language': ['en-US']}\n",
      "{'title': 'Temperature profiles from MBT casts from the GRESHAM from Ocean Weather Station N (OWS-N) in the North Pacific Ocean from 07 February 1967 to 28 February 1967 (NCEI Accession 6700065)', 'accessLevel': 'public', 'accrualPeriodicity': 'irregular', 'contactPoint': {'@type': 'vcard:Contact', 'fn': 'DOC/NOAA/NESDIS/NCEI > National Centers for Environmental Information, NESDIS, NOAA, U.S. Department of Commerce', 'hasEmail': 'mailto:NODC.Services@noaa.gov'}, 'description': 'Bathythermograph data were collected from the GRESHAM within a 1-mile radius of Ocean Weather Station N (3000N 14000W) and in transit. Data were collected by the United States Coast Guard from 07 Februay 1967 to 28 February 1967. The platform was equipped and staffed to observe weather and sea conditions. Data were processed by NODC to the NODC standard Universal Bathythermograph Output (UBT) format. Full format description is available from NODC at www.nodc.noaa.gov/General/NODC-Archive/bt.html.\\n\\nThe UBT file format is used for temperature-depth profile data obtained using the mechanical bathythermograph (MBT) instrument. The maximum depth of MBT observations is approximately 285 m. Therefore, MBT data are useful only in studying the thermal structure of the upper layers of the ocean. Cruise information, date, position, and time are reported for each observation. The data record comprises pairs of temperature-depth values. Temperature data in this file are recorded at uniform 5 m depth intervals.', 'identifier': 'gov.noaa.nodc:6700065', 'keyword': ['DOC/NOAA/NESDIS/NODC > National Oceanographic Data Center', 'NESDIS', 'NOAA', 'U.S. Department of Commerce', 'DOC/NOAA/NESDIS/NCEI > National Centers for Environmental Information', 'NESDIS', 'NOAA', 'U.S. Department of Commerce', 'oceanography'], 'modified': '2010-12-17', 'publisher': {'@type': 'org:Organization', 'name': 'DOC/NOAA/NESDIS/NCEI > National Centers for Environmental Information, NESDIS, NOAA, U.S. Department of Commerce (Point of Contact)'}, 'spatial': '{\"type\": \"Polygon\", \"coordinates\": [[[-140.2, 29.8], [-124.5, 29.8], [-124.5, 37.0], [-140.2, 37.0], [-140.2, 29.8]]]}', 'temporal': '1967-02-07T00:00:00/1967-02-28T00:00:00', 'distribution': [{'@type': 'dcat:Distribution', 'downloadURL': 'http://accession.nodc.noaa.gov/6700065', 'mediaType': 'text/html', 'title': 'Details'}, {'@type': 'dcat:Distribution', 'downloadURL': 'http://accession.nodc.noaa.gov/oas/6700065', 'mediaType': 'text/html', 'title': 'Metadata'}, {'@type': 'dcat:Distribution', 'downloadURL': 'http://accession.nodc.noaa.gov/download/6700065', 'mediaType': 'text/html', 'title': 'Download'}, {'@type': 'dcat:Distribution', 'downloadURL': 'ftp://ftp.nodc.noaa.gov/nodc/archive/arc0001/6700065/', 'mediaType': 'text/html', 'title': 'FTP'}, {'@type': 'dcat:Distribution', 'downloadURL': 'http://www.ncei.noaa.gov/', 'mediaType': 'text/html', 'title': 'NOAA National Centers for Environmental Information website'}, {'@type': 'dcat:Distribution', 'downloadURL': 'http://www.nodc.noaa.gov/', 'mediaType': 'text/html', 'title': 'NOAA National Oceanographic Data Center website'}, {'@type': 'dcat:Distribution', 'downloadURL': 'http://www.nodc.noaa.gov', 'mediaType': 'text/html', 'title': 'US National Oceanographic Data Center website'}, {'@type': 'dcat:Distribution', 'downloadURL': 'http://gcmd.gsfc.nasa.gov/learn/keywords.html', 'mediaType': 'text/html', 'title': 'NASA GCMD Keyword Community Page'}], 'bureauCode': ['006:48'], 'programCode': ['006:059'], 'language': ['en-US']}\n"
     ]
    }
   ],
   "source": [
    "with open('C:/Users/user/OneDrive/Documents/M1-APE-DS2E/Infrastructure_de_donnes/NoSQL/data_world.json', 'r') as fp:\n",
    "    Q9 = json.load(fp)\n",
    "\n",
    "# I will delete the key '@type'\n",
    "\n",
    "import tqdm\n",
    "\n",
    "for doc in tqdm.tqdm(Q9):\n",
    "    if '@type' in doc:\n",
    "        del doc['@type']\n",
    "\n",
    "# Now saving it as data_world_cleaned in a json format\n",
    "\n",
    "with open(\"C:/Users/user/OneDrive/Documents/M1-APE-DS2E/Infrastructure_de_donnes/NoSQL/data_world_cleaned.json\", 'w') as fp:\n",
    "    json.dump(Q9, fp)\n",
    "    \n",
    "with open('C:/Users/user/OneDrive/Documents/M1-APE-DS2E/Infrastructure_de_donnes/NoSQL/data_world_cleaned.json', 'r') as fp:\n",
    "    test2= json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a8ad04-8c3e-46e2-82ff-981b687dba89",
   "metadata": {},
   "source": [
    "#### 10: From the data/Chap2/data_world.json file, create the co-occurence matrix between \"accessLevel\" and \"accrualPeriodicity\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c238e437-82c3-4f0a-8cfb-732dbe047003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  29    1    1    3 4961    5]]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load JSON data from the file\n",
    "with open('C:/Users/user/OneDrive/Documents/M1-APE-DS2E/Infrastructure_de_donnes/NoSQL/data_world.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize a dictionary to store co-occurrence counts\n",
    "co_occurrence_counts = {}\n",
    "\n",
    "# Iterate through the data to count co-occurrences\n",
    "for item in data:\n",
    "    access_level = item.get('accessLevel')\n",
    "    accrual_periodicity = item.get('accrualPeriodicity')\n",
    "\n",
    "    # Increment co-occurrence count\n",
    "    co_occurrence_counts[(access_level, accrual_periodicity)] = co_occurrence_counts.get((access_level, accrual_periodicity), 0) + 1\n",
    "\n",
    "# Get unique values of accessLevel and accrualPeriodicity\n",
    "access_levels = set(item.get('accessLevel') for item in data)\n",
    "accrual_periodicities = set(item.get('accrualPeriodicity') for item in data)\n",
    "\n",
    "# Construct the co-occurrence matrix\n",
    "matrix = np.zeros((len(access_levels), len(accrual_periodicities)), dtype=int)\n",
    "\n",
    "# Fill the matrix with co-occurrence counts\n",
    "for i, access_level in enumerate(access_levels):\n",
    "    for j, accrual_periodicity in enumerate(accrual_periodicities):\n",
    "        matrix[i, j] = co_occurrence_counts.get((access_level, accrual_periodicity), 0)\n",
    "\n",
    "# Print the co-occurrence matrix\n",
    "print(matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
